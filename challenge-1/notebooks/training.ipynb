{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Team Name: RAV\n",
    "Team Members: VIGNESH J, ASHWATH VINODKUMAR, RAHUL BHARGAV TALLADA\n",
    "Leaderboard Rank: 50\n",
    "```\n",
    "\n",
    "# Soil Classification Challenge - Training Notebook\n",
    "\n",
    "This notebook contains the training pipeline for the soil classification challenge. It uses a pre-trained vision transformer model with CLIP architecture to extract features from soil images and trains a logistic regression classifier on these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, we'll install the required packages and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!pip install open-clip-torch pandas pillow scikit-learn --quiet\n",
    "\n",
    "import open_clip\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Define the model configuration and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configuration (MODIFY IF NEEDED)\n",
    "MODEL_NAME = \"ViT-H-14\"        # High-performance vision transformer\n",
    "PRETRAINED = \"laion2b_s32b_b79k\"  # Pretraining dataset\n",
    "BATCH_SIZE = 8                 # Reduce to 4 if OOM errors persist\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CLASSES = [\"Alluvial soil\", \"Black Soil\", \"Clay soil\", \"Red soil\"]\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Loading\n",
    "\n",
    "Load the pre-trained CLIP model and preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load model and preprocessor\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    model_name=MODEL_NAME,\n",
    "    pretrained=PRETRAINED\n",
    ")\n",
    "model = model.to(DEVICE).eval()\n",
    "print(f\"Loaded {MODEL_NAME} model with {PRETRAINED} weights\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading\n",
    "\n",
    "Load the training data and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load metadata\n",
    "# Update paths as needed for your environment\n",
    "train_df = pd.read_csv(\"/kaggle/input/soil-classification/soil_classification-2025/train_labels.csv\")\n",
    "print(f\"Loaded {len(train_df)} training samples\")\n",
    "print(train_df.head())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prompt Engineering\n",
    "\n",
    "Create rich text prompts for each soil class to enhance zero-shot classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Enhanced prompt engineering\n",
    "class_prompts = {\n",
    "    \"Alluvial soil\": [\n",
    "        \"A high-resolution photo of alluvial soil: light brown, fine-textured, river-deposited\",\n",
    "        \"Satellite image showing alluvial plains with fertile soil\",\n",
    "        \"Microscopic view of alluvial soil particles\"\n",
    "    ],\n",
    "    \"Black Soil\": [\n",
    "        \"Agricultural black soil with high clay content\",\n",
    "        \"Vertisol soil cracking in dry conditions\",\n",
    "        \"Aerial view of black cotton soil fields\"\n",
    "    ],\n",
    "    \"Clay soil\": [\n",
    "        \"Sticky clay soil with poor drainage\",\n",
    "        \"Cracked clay surface during drought\",\n",
    "        \"Red clay soil with high iron content\"\n",
    "    ],\n",
    "    \"Red soil\": [\n",
    "        \"Lateritic red soil in tropical regions\",\n",
    "        \"Red earth with visible iron oxide deposits\",\n",
    "        \"Terra rossa soil in Mediterranean climate\"\n",
    "    ]\n",
    "}"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Text Embedding Generation\n",
    "\n",
    "Precompute text embeddings for each class prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Precompute text embeddings\n",
    "with torch.no_grad():\n",
    "    text_embeddings = {}\n",
    "    for cls, prompts in class_prompts.items():\n",
    "        embeddings = []\n",
    "        for prompt in prompts:\n",
    "            text = open_clip.tokenize([prompt]).to(DEVICE)\n",
    "            embeddings.append(model.encode_text(text))\n",
    "        text_embeddings[cls] = torch.mean(torch.cat(embeddings), dim=0, keepdim=True)\n",
    "        \n",
    "print(f\"Generated text embeddings for {len(text_embeddings)} classes\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Image Embedding Function\n",
    "\n",
    "Define a memory-optimized function to generate image embeddings in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Memory-optimized embedding generator\n",
    "def get_image_embeddings(image_paths):\n",
    "    \"\"\"Batch processing to prevent OOM errors\"\"\"\n",
    "    embeddings = []\n",
    "    for i in range(0, len(image_paths), BATCH_SIZE):\n",
    "        batch_paths = image_paths[i:i+BATCH_SIZE]\n",
    "        batch = torch.stack([preprocess(Image.open(p).convert(\"RGB\")) for p in batch_paths])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch = batch.to(DEVICE)\n",
    "            batch_emb = model.encode_image(batch)\n",
    "            embeddings.append(batch_emb.cpu().numpy())\n",
    "        \n",
    "        # Explicit memory cleanup\n",
    "        del batch, batch_emb\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return np.concatenate(embeddings)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Extraction\n",
    "\n",
    "Extract features from training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare training data\n",
    "# Update paths as needed for your environment\n",
    "train_images = [Path(\"/kaggle/input/soil-classification/soil_classification-2025/train\")/img_id for img_id in train_df.image_id]\n",
    "print(f\"Extracting features from {len(train_images)} images...\")\n",
    "X_train = get_image_embeddings(train_images)\n",
    "y_train = train_df.soil_type.map({cls:i for i, cls in enumerate(CLASSES)}).values\n",
    "print(f\"Feature extraction complete. Shape: {X_train.shape}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Training\n",
    "\n",
    "Train a logistic regression classifier on the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train classifier\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
    ")\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight=\"balanced\",\n",
    "    C=0.1,\n",
    "    penalty=\"l2\",\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_train_split, y_train_split)\n",
    "val_accuracy = clf.score(X_val_split, y_val_split)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.2%}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Saving\n",
    "\n",
    "Save the trained model and embeddings for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save model and embeddings\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save classifier\n",
    "with open('../models/classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "\n",
    "# Save text embeddings\n",
    "torch.save(text_embeddings, '../models/text_embeddings.pt')\n",
    "\n",
    "print(\"Model and embeddings saved successfully\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training Summary\n",
    "\n",
    "Summarize the training process and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Training Summary:\")\n",
    "print(f\"- Model: {MODEL_NAME} with {PRETRAINED} weights\")\n",
    "print(f\"- Training samples: {len(X_train_split)}\")\n",
    "print(f\"- Validation samples: {len(X_val_split)}\")\n",
    "print(f\"- Validation accuracy: {val_accuracy:.2%}\")\n",
    "print(f\"- Classes: {CLASSES}\")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

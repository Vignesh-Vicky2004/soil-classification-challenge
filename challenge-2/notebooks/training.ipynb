{
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "```\nAuthor: Annam.ai IIT Ropar\nTeam Name: RAV\nTeam Members: VIGNESH J, ASHWATH VINODKUMAR, RAHUL BHARGAV TALLADA\nLeaderboard Rank: 13\n```\n\n# This is the notebook used for training the soil classification model."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Setup and Configuration\n\nInstalling required packages and importing necessary libraries for the soil classification task."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "!pip install open-clip-torch pandas pillow scikit-learn matplotlib tqdm --quiet\n\nimport sys\nimport os\nsys.path.append('../src')\n\nimport open_clip\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Import custom utilities\nfrom preprocessing import load_and_preprocess_image, batch_preprocess_images, normalize_embeddings\nfrom postprocessing import analyze_predictions, plot_confusion_matrix\n\n# Configuration\nMODEL_NAME = \"ViT-H-14\"\nPRETRAINED = \"laion2b_s32b_b79k\"\nBATCH_SIZE = 8\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nRANDOM_SEED = 42\n\nprint(f\"Using device: {DEVICE}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Data Loading\n\nLoading the training dataset for the soil classification task."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Load metadata\nDATA_DIR = Path(\"../data\")\nif not DATA_DIR.exists():\n    # For Kaggle environment\n    DATA_DIR = Path(\"/kaggle/input/soil-classification-part-2/soil_competition-2025\")\n\ntrain_df = pd.read_csv(DATA_DIR / \"train_labels.csv\")\n\nprint(f\"Training samples: {len(train_df)}\")\nprint(f\"Sample data:\")\ndisplay(train_df.head())"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Model Loading\n\nLoading the CLIP model and preprocessing transforms."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Load model and preprocessor\nprint(f\"Loading {MODEL_NAME} model with {PRETRAINED} weights...\")\nmodel, _, preprocess = open_clip.create_model_and_transforms(\n    model_name=MODEL_NAME,\n    pretrained=PRETRAINED\n)\nmodel = model.to(DEVICE).eval()\nprint(\"Model loaded successfully!\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Text Prompts for Zero-Shot Classification\n\nDefining text prompts for soil and non-soil categories to use with CLIP model."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Binary classification prompts\nsoil_prompts = [\n    \"A photograph of soil, earth, dirt, or ground surface\",\n    \"Agricultural soil in a field or garden\",\n    \"Natural earth surface with visible soil texture\",\n    \"Close-up view of soil, dirt, or earth material\",\n    \"Ground surface showing soil composition\",\n    \"Farmland soil or agricultural earth\",\n    \"Natural soil formation with organic matter\"\n]\n\nnon_soil_prompts = [\n    \"A photograph with no soil, dirt, or earth visible\",\n    \"Indoor scene without any ground or soil\",\n    \"Water, sky, buildings, or other non-soil objects\",\n    \"Concrete, asphalt, or artificial surfaces\",\n    \"Plants, rocks, or objects without visible soil\",\n    \"Urban environment without natural earth\",\n    \"Clean surfaces without dirt or soil material\"\n]\n\nprint(f\"Number of soil prompts: {len(soil_prompts)}\")\nprint(f\"Number of non-soil prompts: {len(non_soil_prompts)}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Computing Text Embeddings\n\nPrecomputing text embeddings for the soil and non-soil categories."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Precompute text embeddings for binary classification\nwith torch.no_grad():\n    # Soil embeddings\n    soil_embeddings = []\n    for prompt in soil_prompts:\n        text = open_clip.tokenize([prompt]).to(DEVICE)\n        soil_embeddings.append(model.encode_text(text))\n    soil_text_embedding = torch.mean(torch.cat(soil_embeddings), dim=0, keepdim=True)\n    \n    # Non-soil embeddings\n    non_soil_embeddings = []\n    for prompt in non_soil_prompts:\n        text = open_clip.tokenize([prompt]).to(DEVICE)\n        non_soil_embeddings.append(model.encode_text(text))\n    non_soil_text_embedding = torch.mean(torch.cat(non_soil_embeddings), dim=0, keepdim=True)\n    \n    # Normalize embeddings\n    soil_text_embedding = normalize_embeddings(soil_text_embedding)\n    non_soil_text_embedding = normalize_embeddings(non_soil_text_embedding)\n\nprint(\"Text embeddings computed successfully!\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Preparing Training Data\n\nPreparing the training data for the soil classification model."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Prepare training data paths\ntrain_images = [DATA_DIR / \"train\" / img_id for img_id in train_df.image_id]\nprint(f\"Total training images: {len(train_images)}\")\n\n# Display a few sample images\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\nfor i, ax in enumerate(axes):\n    if i < len(train_images):\n        img = Image.open(train_images[i])\n        ax.imshow(img)\n        ax.set_title(f\"Sample {i+1}\")\n        ax.axis('off')\nplt.tight_layout()\nplt.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Computing Image Embeddings\n\nComputing embeddings for the training images in batches to prevent memory issues."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Function to get image embeddings in batches\ndef get_image_embeddings(image_paths):\n    \"\"\"Batch processing to prevent OOM errors\"\"\"\n    embeddings = []\n    for i in tqdm(range(0, len(image_paths), BATCH_SIZE), desc=\"Processing batches\"):\n        batch_paths = image_paths[i:i+BATCH_SIZE]\n        batch = torch.stack([preprocess(Image.open(p).convert(\"RGB\")) for p in batch_paths])\n        \n        with torch.no_grad():\n            batch = batch.to(DEVICE)\n            batch_emb = model.encode_image(batch)\n            embeddings.append(batch_emb.cpu().numpy())\n        \n        # Memory cleanup\n        del batch, batch_emb\n        torch.cuda.empty_cache()\n    \n    return np.concatenate(embeddings)\n\n# Compute embeddings for training images\nprint(\"Computing image embeddings for training data...\")\nX_train = get_image_embeddings(train_images)\nprint(f\"Computed embeddings shape: {X_train.shape}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Training Data Preparation\n\nPreparing labels for the training data. Since all training images contain soil, they are labeled as 1."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# All training images are soil (label = 1)\n# For a more robust classifier, you might want to add negative examples (non-soil images)\ny_train = np.ones(len(X_train), dtype=int)  # All are soil = 1\n\nprint(f\"Training samples: {len(X_train)} (all labeled as soil=1)\")\n\n# Split data for validation\nX_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n    X_train, y_train, test_size=0.2, random_state=RANDOM_SEED\n)\n\nprint(f\"Training set: {len(X_train_split)} samples\")\nprint(f\"Validation set: {len(X_val_split)} samples\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Zero-Shot Classification Function\n\nImplementing the zero-shot classification function to predict whether an image contains soil or not."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Function to predict if an image contains soil using zero-shot classification\ndef predict_is_soil(image_path, threshold=0.0):\n    \"\"\"\n    Predict if image contains soil (1) or not (0)\n    Uses zero-shot CLIP classification\n    \"\"\"\n    image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(DEVICE)\n    \n    with torch.no_grad():\n        # Get image features\n        image_features = model.encode_image(image)\n        image_features = normalize_embeddings(image_features)\n        \n        # Get text features\n        soil_features = soil_text_embedding.to(DEVICE)\n        non_soil_features = non_soil_text_embedding.to(DEVICE)\n        \n        # Calculate similarities\n        soil_similarity = (image_features @ soil_features.T).item()\n        non_soil_similarity = (image_features @ non_soil_features.T).item()\n        \n        # Use softmax to get probabilities\n        logits = torch.tensor([non_soil_similarity, soil_similarity])\n        probs = torch.softmax(logits, dim=0)\n        \n        # Return 1 if soil probability > non-soil probability + threshold\n        is_soil = 1 if probs[1] > (probs[0] + threshold) else 0\n        \n    return is_soil, probs[1].item()  # Return prediction and soil probability\n\n# Simpler version - just compare similarities\ndef predict_is_soil_simple(image_path):\n    \"\"\"Simpler version - just compare soil vs non-soil similarity\"\"\"\n    image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(DEVICE)\n    \n    with torch.no_grad():\n        image_features = model.encode_image(image)\n        image_features = normalize_embeddings(image_features)\n        \n        soil_features = soil_text_embedding.to(DEVICE)\n        non_soil_features = non_soil_text_embedding.to(DEVICE)\n        \n        soil_sim = (image_features @ soil_features.T).item()\n        non_soil_sim = (image_features @ non_soil_features.T).item()\n        \n        # Return 1 if more similar to soil than non-soil\n        return 1 if soil_sim > non_soil_sim else 0"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Testing on Training Images\n\nTesting the zero-shot classification function on a few training images."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Test on a few training images to verify\nprint(\"\\nTesting on a few training images (should all be 1 since they contain soil):\")\nsample_images = train_images[:5]\n\nfig, axes = plt.subplots(1, 5, figsize=(20, 4))\nfor i, (img_path, ax) in enumerate(zip(sample_images, axes)):\n    prediction, confidence = predict_is_soil(img_path)\n    simple_pred = predict_is_soil_simple(img_path)\n    \n    # Display image and prediction\n    img = Image.open(img_path)\n    ax.imshow(img)\n    ax.set_title(f\"Pred={prediction} (conf={confidence:.3f})\\nSimple={simple_pred}\")\n    ax.axis('off')\n    \n    print(f\"Image {i+1}: Advanced={prediction} (conf={confidence:.3f}), Simple={simple_pred}\")\n\nplt.tight_layout()\nplt.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Model Evaluation\n\nEvaluating the model on the validation set."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Since we only have positive examples in our training data,\n# we'll rely on zero-shot classification for the final model\nprint(\"Using zero-shot classification as our final model\")\n\n# Save the text embeddings for inference\nembeddings_output = {\n    \"soil_embedding\": soil_text_embedding.cpu().numpy(),\n    \"non_soil_embedding\": non_soil_text_embedding.cpu().numpy()\n}\n\n# Save embeddings using numpy\nnp.save(\"../model/text_embeddings.npy\", embeddings_output)\nprint(\"Text embeddings saved for inference\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Conclusion\n\nThe training process is complete. We've created a zero-shot classifier using CLIP that can determine whether images contain soil or not. The model will be used for inference on the test set."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print(\"Training complete! The model is ready for inference.\")\nprint(\"Please run the inference.ipynb notebook to generate predictions on the test set.\")"
        }
    ]
}
